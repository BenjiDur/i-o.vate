{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i/o.vate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 - Environment; Sourcing and Scraping our Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project idea, in line with a tight deadline, requires to restrict the heaps of data at some point. A proof of concept that is able to produce valid outcomes is the target for a showcase, and should spark the motivation to continue producing findings. We have found that a solid body of investigation should be provided by WIPO as the governing body (the World Intellectual Property Organization). Hong Kong being part of the Paris convention is included in the territories governed by WIPO. Which allows us to define and easily locate patents valid in Hong Kong by their WIPO numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, they certainly require some sort of categorization method, and this in English language. As we have seen, the Hong Kong Department of intellectual propery provides a weekly journal with updates on patent filings in the Special Administrative region in PDF format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are facing an initial problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What environment to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a duo's effort and we wouldn't want to rely on methods such as Dropbox for file exchanges, there had to be a solution that fulfills several requirements:\n",
    "\n",
    "- decentralized\n",
    "- remotely accessible\n",
    "- ideally bringing enough computational power to the table\n",
    "\n",
    "This way we made sure we'd operate on the same environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually a Linux environment on cloud servers was chosen, and a machine learning suite deployed to this. Specifications:\n",
    "\n",
    "- Ubuntu 18.04\n",
    "- 4GB RAM, 80GB disk size, 2 virtual CPUs has proven to be a good compromise\n",
    "- Headless operation kept it lean and quick\n",
    "- SSH connection / FTP for file exchange and running scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software:\n",
    "\n",
    "- Python 3\n",
    "- pdf parsers\n",
    "- Selenium, headless Firefox\n",
    "- Pandas, Seaborn\n",
    "- NLTK\n",
    "- Keras / Word2Vec\n",
    "- Gensim / Doc2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting our Hong Kong Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We reference hereby to the file scrape_convert.py.\n",
    "This notebook serves as an explanation of our method, step by step, while the script file is execution-ready***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hong Kong Intellectual Property Department releases a weekly journal with patent information. As these are all PDF files, we can handle them easily by downloading and extracting text on our machine. The following elements were necessary from a concept point of view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- downloader for a massive amount of PDFs. Challenge: there is not a clear pattern on the publication dates. \n",
    "- file handling: save a large quantity of files distinctively for further processing\n",
    "- data extraction. Challenge: separate and structure information, find extraction patterns\n",
    "- parsing: create concise dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, we have noticed that while the titles of these patents have been published here, we are missing the abstracts (that will make up our actual corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some libraries. Since we are using Python 3.x, note that pdfminer can be installed via pip under \"pdfminer.six\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import io\n",
    "import datetime\n",
    "import os\n",
    "import pdfminer\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peculiar problem is that not all documents have been published on a Friday. So we want a hassle-free method of trying different dates. Let's create a list of the past 10,000 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "date_list = [today - datetime.timedelta(days=x) for x in range(0, 10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to convert the datetimes in a way that they fit our links. It has been a pleasure to obtain the PDFs, since the URLs to the issues were bearing the publication date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ipsearch.ipd.gov.hk/hkipjournal/29112013/Patent_29112013.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of storing these files have been changed over the years, as well as their encodings. For this documentation we are only following one path, since the others only required adjustments in the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Chinese characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since WIPO and Basic Law in Hong Kong require the publication in English language, we recognize Chinese in this exercise as a supplement. It is, all things considered, also easier to process given the proven approaches to NLP on English corpora. Here's our function that handles the separation of Chinese from English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnonChinese(context):\n",
    "    filtrate = re.compile(u'[\\u4e00-\\u9fff]') # Chinese unicode range\n",
    "    context = filtrate.sub(r'', context) # remove all Chinese characters\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter-based Division of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we see that the text is divided in three parts: requests, granted standard patents and short term (for HK first applications). The formatting here is different depending on those, so we are extracting these parts separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_divider():\n",
    "\n",
    "    text = text[text.find('Requests to Record Published (Arranged by International\\nPatent Classification)'):]\n",
    "\n",
    "    # Part 1 : Requests to Record Designated Patent Applications published under section 20 of the Patents Ordinance\n",
    "    part1 = text[:text.find(\"Requests to Record Published (Arranged by Publication No.)\")]\n",
    "\n",
    "    # Part 2 : Granted Standard Patents published under section 27 of the Patents Ordinance\n",
    "    part2 = text[text.find(\"Standard Patents Granted (Arranged by International Patent\\nClassification)\"):text.find(\"Standard Patents Granted (Arranged by Publication No.)\")]\n",
    "\n",
    "    # Part 3 : Granted Short-term Patents published under section 118 of the Patents Ordinance\n",
    "    part3 = text[text.find(\"Short-term Patents Granted (Arranged by International Patent\\nClassification)\"):text.find(\"Short-term Patents Granted (Arranged by Publication No.)\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring INID codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at how every single patent is annotated, we see that for each item there are some feature representations flagged via INID codes (\"Internationally agreed Numbers for the Identification of (bibliographic) Data\"). Since we want to grab these information for each patent, it is a good idea to parse them into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionnary with keys representing the INID Codes\n",
    "def generate_allindex():\n",
    "    d = {}\n",
    "    d['51'] = ''\n",
    "    d['11'] = ''\n",
    "    d['11A'] = ''\n",
    "    d['13'] = ''\n",
    "    d['25'] = ''\n",
    "    d['21'] = ''\n",
    "    d['22'] = ''\n",
    "    d['86'] = ''\n",
    "    d['86A'] = ''\n",
    "    d['87'] = ''\n",
    "    d['30'] = ''\n",
    "    d['62'] = ''\n",
    "    d['54'] = ''\n",
    "    d['73N'] = ''\n",
    "    d['73C'] = ''\n",
    "    d['71N'] = ''\n",
    "    d['71C'] = ''\n",
    "    d['72'] = ''\n",
    "    d['74N'] = ''\n",
    "    d['74C'] = ''\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing INID features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part follows, as each field's values are represented in a different way. One comfortable observation: our patents are clearly separated by a line of underscores. This will help us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser51(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        fiftyone = \"\".join(entry.split('\\n'))\n",
    "        fiftyone = \"\".join(re.findall(r'\\b[A-Z0-9]{4}\\b|$', entry))\n",
    "        return fiftyone\n",
    "\n",
    "def parser11(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return re.findall(r'[\\d]{7}[*]?|$', entry)[0]\n",
    "\n",
    "def parser11A(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return re.findall(\"[A-Z]{2}[\\d]+.+[\\d]*[A-Z]|$\", entry)[0]\n",
    "\n",
    "def parser13(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return re.findall(r'\\b[A-Z]{1}\\b|$' ,entry)[0]\n",
    "\n",
    "def parser25(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(entry.split('\\n')[0].strip())\n",
    "\n",
    "def parser21(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(entry.strip())\n",
    "\n",
    "def parser22(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(entry.split(\"\\n\")[0]).strip()\n",
    "\n",
    "def parser86(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(entry.split('\\n')[0].strip())\n",
    "\n",
    "def parser86A(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(re.findall(r'[A-Z]+.[A-Z]{2}\\d+.\\d+', entry))\n",
    "\n",
    "def parser87(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \"\".join(entry.split('\\n')[0].strip())\n",
    "\n",
    "def parser30(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \",\".join(re.findall(r'\\d{2}.\\d{2}.\\d{4}\\s+\\w{2}.+\\d+.[A-Z0-9,]+', entry))\n",
    "\n",
    "def parser62(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return \",\".join(re.findall(r'\\d{2}.\\d{2}.\\d{4}\\s+\\w{2}.\\d+.[A-Z0-9]+',entry))\n",
    "\n",
    "def parser54(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    return ''.join(re.findall(r'[A-Z]+[^A-Za-z]', entry)).strip()\n",
    "\n",
    "def parser73N(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        CompanyName = entry.split('\\n')[0]\n",
    "        return CompanyName\n",
    "\n",
    "def parser73C(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        Country = re.sub(r'[^a-zA-Z\\s]','', entry)\n",
    "        Country = \"\".join(Country.strip().split('\\n')[-2:])\n",
    "        return Country\n",
    "\n",
    "def parser71N(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        CompanyName = entry.split('\\n')[0]\n",
    "        return CompanyName\n",
    "\n",
    "def parser71C(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        Country = re.sub(r'[^a-zA-Z\\s]','', entry)\n",
    "        Country = \"\".join(Country.strip().split(\"\\n\")[-2:])\n",
    "        return Country\n",
    "\n",
    "def parser72(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        Name = re.sub(r'[^a-zA-Z\\s,]','', entry)\n",
    "        Name = \"\".join(Name.split('\\n')).strip()\n",
    "        return Name\n",
    "\n",
    "def parser74N(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        CompanyName = entry.split('\\n')[0].strip()\n",
    "        return CompanyName\n",
    "\n",
    "def parser74C(entry):\n",
    "    if not entry:\n",
    "        return np.nan\n",
    "    else:\n",
    "        Country = re.sub(r'[^a-zA-Z\\s]','', entry)\n",
    "        Country = Country.strip().split('\\n')[-1].strip()\n",
    "        return Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our row data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a list of patents and their INID/Information pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_patent(part):\n",
    "    dlist =[]\n",
    "    for patent in part:\n",
    "        d = generate_allindex()\n",
    "        for entry in patent:\n",
    "            if entry.startswith('[51]'):\n",
    "                d['51'] = parser51(entry)\n",
    "            elif entry.startswith('[11]'): \n",
    "                d['51'] += parser51(entry)\n",
    "                d['11'] = parser11(entry)\n",
    "                d['11A'] = parser11A(entry)\n",
    "            elif entry.startswith('[13]'):\n",
    "                d['13'] = parser13(entry[5:])\n",
    "            elif entry.startswith('[25]'):\n",
    "                d['25'] = parser25(entry[5:])\n",
    "            elif entry.startswith('[21]'):\n",
    "                d['21'] = parser21(entry[5:])\n",
    "            elif entry.startswith('[22]'):\n",
    "                d['22'] = parser22(entry[5:])\n",
    "            elif entry.startswith('[86]'):\n",
    "                d['86'] = parser86(entry[5:])\n",
    "                d['86A'] = parser86A(entry)\n",
    "            elif entry.startswith('[87]'):\n",
    "                d['87'] = parser86(entry[5:])\n",
    "            elif entry.startswith('[30]'):\n",
    "                d['30'] = parser30(entry[5:])\n",
    "            elif entry.startswith('[62]'):\n",
    "                d['62'] = parser62(entry)\n",
    "            elif entry.startswith('[54]'):\n",
    "                d['54'] = parser54(entry)\n",
    "            elif entry.startswith('[73]'):\n",
    "                d['73N'] = parser73N(entry[5:])\n",
    "                d['73C'] = parser73C(entry)\n",
    "            elif entry.startswith('[71]'):\n",
    "                d['71N'] = parser71N(entry[5:])\n",
    "                d['71C'] = parser71C(entry)\n",
    "            elif entry.startswith('[72]'):\n",
    "                d['72'] = parser72(entry[5:])\n",
    "            elif entry.startswith('[74]'):\n",
    "                d['74N'] = parser74N(entry[5:])\n",
    "                d['74C'] = parser74C(entry)\n",
    "        dlist.append(d)\n",
    "    return dlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push it into a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to populate a dataframe with this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes(part):\n",
    "    df = pd.DataFrame(get_list_patent(part))\n",
    "    df = df[['51','11','11A','13','25','21','22','86','86A','87','30','62','54','73N','73C','71N','71C','72','74N','74C']]\n",
    "    df = df[:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember it looked different, depending on the patent type (request, grant, short-term grant?) We need to treat them differently before we call this function, so here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1:  \"Requests to Record Designated Patent Applications published under section 20 of the Patents Ordinance\"\n",
    "def part_1_creator(part1):\n",
    "    # get rid of the Header on each page\n",
    "    part1 = part1.replace(\"Journal No.:\",'')\n",
    "    part1 = part1.replace('\\x0c','')\n",
    "    part1 = part1.replace(\"Section Name:\",\"\")\n",
    "    part1 = part1.replace('Publication Date:','')\n",
    "    part1 = part1.replace('Requests to Record Published (Arranged by International\\nPatent Classification)','')\n",
    "    part1 = part1.replace('Arranged by International Patent Classification', '')\n",
    "    line = \"_______________________________________________________________________\"\n",
    "    part1 = part1.split(line)\n",
    "    patent_part1 = []\n",
    "    for patent in part1:\n",
    "        patent_part1.append(re.findall(r'\\[\\d+\\][^\\[]*', patent, re.S))\n",
    "    df1 = dataframes(patent_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2:  \"Granted Standard Patents published under section 27 of the Patents Ordinance\"\n",
    "def part_2_creator(part2):\n",
    "    # get rid of the Header on each page\n",
    "    part2 = part2.replace(\"Journal No.:\",'')\n",
    "    part2 = part2.replace('\\x0c','')\n",
    "    part2 = part2.replace(\"Section Name: ()\",\"\")\n",
    "    part2 = part2.replace('Publication Date:','')\n",
    "    part2 = part2.replace('Standard Patents Granted (Arranged by International Patent\\nClassification)','')\n",
    "    part2 = part2.replace('Arranged by International Patent Classification', '')\n",
    "    part2 = part2.replace('Section Name: ()  Standard Patents Granted ()', '')\n",
    "\n",
    "    part2 = part2.split(line)\n",
    "    patent_part2 = []\n",
    "    for patent in part2:\n",
    "        patent_part2.append(re.findall(r'\\[\\d+\\][^\\[]*', patent, re.S))\n",
    "\n",
    "    df2 = dataframes(patent_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3:  \"Granted Short-term Patents published under section 118 of the Patents Ordinance\"\n",
    "def part_3_creator(part3):\n",
    "    # get rid of the Header on each page\n",
    "    part3 = part3.replace(\"Journal No.: 803\",'')\n",
    "    part3 = part3.replace('\\x0c','')\n",
    "    part3 = part3.replace(\"Section Name: ()\",\"\")\n",
    "    part3 = part3.replace('Publication Date: 17-08-2018','')\n",
    "    part3 = part3.replace('Short-term Patents Granted (Arranged by International Patent\\nClassification)','')\n",
    "    part3 = part3.replace('Arranged by International Patent Classification', '')\n",
    "    part3 = part3.replace('Section Name: ()  Short-term Patents Granted ()', '')\n",
    "\n",
    "    part3 = part3.split(line)\n",
    "    patent_part3 = []\n",
    "    for patent in part3:\n",
    "        patent_part3.append(re.findall(r'\\[\\d+\\][^\\[]*', patent, re.S))\n",
    "\n",
    "    df3 = dataframes(patent_part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the three parts, we'd have them glued together, and save our dataframe for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FINAL DATAFRAME :\n",
    "def final_Dataframe():\n",
    "    final1 = pd.concat([df1,df2,df3],keys=['Type1','Type2','Type3'])\n",
    "    final = final1.reset_index()\n",
    "    final.drop(['level_1'], inplace = True, axis=1)\n",
    "    final = final.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    final.rename(columns={'level_0': 'Patent Type'}, inplace=True)\n",
    "    return final\n",
    "df_final = final_Dataframe()\n",
    "print(df_final)\n",
    "save_frame = open('%s.pickle' % keydate,'wb')\n",
    "pickle.dump(df_final,save_frame)\n",
    "save_frame.close()\n",
    "print(\"file saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to pack all these elements into a for loop that fetches the PDFs for each issue and performs these operatations. We end up with a big dataframe comprising all patents in a structured way. We had a script running on our server to handle this task that consisted of all the above elements. This took about one night's time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
